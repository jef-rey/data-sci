---
title: "Part 2"
output:
  html_document:
    df_print: paged
---
## Picking up where we left off

First, we are going to want to pull all our data from part 1 and save both as a new file

```{r echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, results='hide'}
include <- function(library_name){
  if( !(library_name %in% installed.packages()) )
    install.packages(library_name) 
  library(library_name, character.only=TRUE)
}
include("tidyverse")
include("knitr")
include("rvest")
include("ggplot2")
purl("p1.Rmd", output = "p1.r")
source("p1.r")
```

We can pick up where we left off by printing the head of the `Traffic Crime` table, just to make sure it's there.
```{r}
head(Traffic_crime)
```
Good, it's there.

# Adding a second source

Now, I want to pull in weather data from the same area as the crimes. We're going to continue to focus on the Denver area, and we're going to scrape the web for data concerning the weather during the same time period, 2014, and 2018-2019.
My hope is that we can find some sort of relation between the amount of incidents and the temperature.


We're going to use the library `riem` to get historical weather data for the Denver area

```{r}
#installing riem
#install.packages("riem")
library("devtools")
install_github("ropensci/riem")
```

```{r}
# load riem
library("riem")
#this gives us a table of all riem networks we can use.
# i was able to find colorado in this this, so that's what we'll use
#riem_networks()
riem_stations("CO_ASOS")
```
Now that we have which station we'd like to pull data from, and referencing the majority of dates from the `Traffic_crimes` table, we're going to want to pull weather information from those corresponding dates.
```{r}
wx_19 <- riem_measures("DEN", date_start = "2019-01-01", date_end = "2019-06-30")
wx_18 <- riem_measures("DEN", date_start = "2018-01-01", date_end = "2018-12-31")
wx_14 <- riem_measures("DEN", date_start = "2014-01-01", date_end = "2014-12-31")
```

If we look at the data, we notice that the temperature is only take at 7 minutes to the new hour, so we have a lot of rows that will need tidying. We also don't need to worry about `relative humidity, wind speed in knots, pressure altimeter in inches, sea level pressure in millibar` or `sky level coverage`, so we have quite a bit we can disregard.

We're going to write a function, much like the one above, that will result in our data being tidied up a bit.
```{r}
omit_na <- function(data, desiredCols) {
  completeVec <- complete.cases(data[, desiredCols])
  return(data[completeVec, ])
}
wx_19 <- omit_na(wx_19, "tmpf")
wx_18 <- omit_na(wx_18, "tmpf")
wx_14 <- omit_na(wx_14, "tmpf")

```
This omitted all rows in `tmpf` that _didn't_ have a value. Now we can delete the rows we won't need.
We know that we're not going to bother with anything other than the first 5 and the 30th columns, so we can just keep those rows.
I think we should keep the temperature _and_ the windchill factor (labelled as _feel_) because that gives you a good impression of how cold it was and how cold it actually felt, which, if you dear reader, has ever had to experience a bad wind chill, will know that it can feel substantially different.


```{r}
wx_19 <- wx_19 %>% select(-31, -(6:29))
wx_18 <- wx_18 %>% select(-31, -(6:29))
wx_14 <- wx_14 %>% select(-31, -(6:29))
test_wx_19 <- wx_19

```

Now we are left with some tidier data that we can play around with more easily. We have dates and times and temperatures and windchill factors.
Let's see if we can plot this to a graph.
```{r}
#generic plot for temperature that includes labels and set for line instead of points
temp_19 <- plot(wx_19$valid, wx_19$tmpf, xlab="Months", ylab="temperature in F", main = "Temperatures recorded hourly for Denver, CO", type = "l")

```

Now, that's incredibly generic but we _can_ get a generic idea of what's going on. It's colder in January than July, and as the year progresses, the temperature rises.
We can also look at windchill in this same way.

```{r}
#generic plot for windchill that includes labels and set for line instead of points
chill <- plot(wx_19$valid, wx_19$feel, xlab="Months", ylab="windchill in F", main = "Wind Chill Temperature recorded hourly for Denver, CO", type = "l")

```


Let's try something more sophisticated. I want to see how both of those graphs would look overlayed on one another. To do this, we'll use a robust library called `ggplot2`. 
```{r}
# jan-jul 2019
g_19 <- ggplot() + geom_line(aes(x=wx_19$valid, y=wx_19$tmpf), colour= "red")+ geom_line(aes(x=wx_19$valid, y=wx_19$feel), colour= "blue")
g_19 + ggtitle("temperature in denver, co from january to july 2019") + labs(x="months", y="temp in f (in red), windchill (in blue)")
```
We can do the same for the other tables we have as well.

```{r}
#2018
g_18 <- ggplot() + geom_line(aes(x=wx_18$valid, y=wx_18$tmpf), colour= "red")+ geom_line(aes(x=wx_18$valid, y=wx_18$feel), colour= "blue")
g_18 + ggtitle("temperature in denver, co from for 2018") + labs(x="months", y="temp in f (in red), windchill (in blue)")

```

```{r}
#2014
g_14 <- ggplot() + geom_line(aes(x=wx_14$valid, y=wx_14$tmpf), colour= "red")+ geom_line(aes(x=wx_14$valid, y=wx_14$feel), colour= "blue")
g_14 + ggtitle("temperature in denver, co for 2014") + labs(x="months", y="temp in f (in red), windchill (in blue)")
```
Now let's try looking at some data from `Traffic_crime` alongside our weather data. 
We'll start with 2019, which means we need to parse our `Traffic_crime` data for dates that occur only in 2019
```{r}
tapply(Traffic_crime$REPORTED_DATE, Traffic_crime$OFFENSE_TYPE_ID,  FUN = function(x) length(unique(x)))

```

This let's us see how many different types of offenses there are, but we need to seperate based on date.

I have to create a function to make a new table that will combine the traffic crime and the weather data. I'm going to call it `TCWX` for "traffic crime and weather data"
```{r}
#creating new table from old
TCWX <- Traffic_crime
#splitting time and date to be easier to work with
TCWX$date <- format(as.POSIXct(TCWX$REPORTED_DATE, format="%Y-%m-%d %H:%M:%S"), "%Y-%m-%d")
TCWX$time <- format(as.POSIXct(TCWX$REPORTED_DATE, format="%Y-%m-%d %H:%M:%S"), "%H:%M:%S")
#getting rid of columns we don't need
TCWX$OFFENSE_CODE_EXTENSION <- NULL
TCWX$LAST_OCCURRENCE_DATE <- NULL
TCWX$GEO_X <- NULL
TCWX$GEO_Y <- NULL
#since we have the incident address, i don't feel like we need the lat and lon
#we really only need one or the other
TCWX$GEO_LON <- NULL
TCWX$GEO_LAT <- NULL
TCWX$FIRST_OCCURRENCE_DATE <- NULL
TCWX$DISTRICT_ID <- NULL
TCWX$PRECINCT_ID <- NULL
TCWX$OFFENSE_CATEGORY_ID <- NULL
#now that i have the date and time separated, i can get rid of REPORTED_DATE
TCWX$REPORTED_DATE <- NULL
```

Let's create a table to run a model on that only incorporates a specific date range

```{r}
TCWX_in_prog <- sqldf("select * from TCWX where date >=2018 and date < 2019")
```

Now I need to separate time and date for `wx_18` in the same way that we did for `TCWX`

```{r}
#we need to separate the weather data's date and time just like TCWX so that I can merge this with TCWX
wx_18$date <- format(as.POSIXct(wx_18$valid, format="%Y-%m-%d %H:%M:%S"), "%Y-%m-%d")
wx_18$time <- format(as.POSIXct(wx_18$valid, format="%Y-%m-%d %H:%M:%S"), "%H:%M:%S")
#then i can delete the valid column, as we don't need it anymore
wx_18$valid <- NULL


```

Now we need to merge these two tables
```{r}
#omit NA values

TCWX_18 <- merge(TCWX_in_prog, wx_18, by="date", all=TRUE)
TCWX_18 <- na.omit(TCWX_18)
```

Once we have our new table with only the data we need, we want to run a linear model to test cross-validation. That is, we want to randomly split the data into two sets, train the model, and then test the model. This will give us a standard deviation and a mean squared error rate.
It essentially means that we can use our new data to determine if there is any correlation on our original data.


```{r}
correlating <- lm(data=TCWX_18, formula=IS_CRIME~tmpf+feel+lat+lon+date)
#summarise
summary(correlating)
```

We can conclude that none of this data are clear predictors of traffic crime. The numbers are absolutely horrible, and I expected this only after I began to work with the data. I chalk this up to data science being very hard.

In Part 3, I will analyze my findings further!

